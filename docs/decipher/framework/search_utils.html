<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>decipher.framework.search_utils API documentation</title>
<meta name="description" content="Functions used by deCiPher search" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cse.google.com/cse.js?cx=017837193012385208679:pey8ky8gdqw"></script>
<style>.gsc-control-cse {padding:0 !important;margin-top:1em}</style>
<style>.homelink{display:block;font-size:2em;font-weight:bold;color:#555;padding-bottom:.5em;border-bottom:1px solid silver}.homelink:hover{color:inherit}.homelink img{max-width:20%;max-height:5em;margin:auto;margin-bottom:.3em}</style>
<link rel="icon" href="https://upload.wikimedia.org/wikipedia/commons/6/61/Searchtool.svg">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>decipher.framework.search_utils</code></h1>
</header>
<section id="section-intro">
<p>Functions used by deCiPher search</p>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/chiefsan/deCiPher/blob/c4ddeace7d1cdb139754478aa0e9415b06e073b4/decipher/framework/search_utils.py#L0-L292" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Functions used by deCiPher search
&#34;&#34;&#34;

from ..framework.schema import engine, Problem, InvertedIndex, TermDictionary
import sqlite3
import nltk
import string
from bs4 import BeautifulSoup
from os import listdir, sep
from os.path import join, isfile, exists
from time import time
from tabulate import tabulate
from collections import defaultdict
from sqlalchemy.orm import scoped_session, sessionmaker
from collections import defaultdict
import numpy as np


def preprocess_text(content: str) -&gt; (list, list):
    &#34;&#34;&#34;
    Preprocesses the text.
    Performs stemming and tokenization. 
    Returns tokens without stopwords and their corresponding indices.
    &#34;&#34;&#34;

    # convert the entire content to lowercase
    content = content.lower()

    # tokenize using the function nltk.word_tokenize
    tokens = nltk.word_tokenize(content)

    # define the stopwords (not downloading it like normal using nltk.download
    # because there was an issue during heroku deployment (resolved now).
    # &#39;$&#39; and &#39;,&#39; are stopwords specifically in this domain (observed when viewing the database)
    stopwords = [
        &#34;i&#34;,
        &#34;me&#34;,
        &#34;my&#34;,
        &#34;myself&#34;,
        &#34;we&#34;,
        &#34;our&#34;,
        &#34;ours&#34;,
        &#34;ourselves&#34;,
        &#34;you&#34;,
        &#34;you&#39;re&#34;,
        &#34;you&#39;ve&#34;,
        &#34;you&#39;ll&#34;,
        &#34;you&#39;d&#34;,
        &#34;your&#34;,
        &#34;yours&#34;,
        &#34;yourself&#34;,
        &#34;yourselves&#34;,
        &#34;he&#34;,
        &#34;him&#34;,
        &#34;his&#34;,
        &#34;himself&#34;,
        &#34;she&#34;,
        &#34;she&#39;s&#34;,
        &#34;her&#34;,
        &#34;hers&#34;,
        &#34;herself&#34;,
        &#34;it&#34;,
        &#34;it&#39;s&#34;,
        &#34;its&#34;,
        &#34;itself&#34;,
        &#34;they&#34;,
        &#34;them&#34;,
        &#34;their&#34;,
        &#34;theirs&#34;,
        &#34;themselves&#34;,
        &#34;what&#34;,
        &#34;which&#34;,
        &#34;who&#34;,
        &#34;whom&#34;,
        &#34;this&#34;,
        &#34;that&#34;,
        &#34;that&#39;ll&#34;,
        &#34;these&#34;,
        &#34;those&#34;,
        &#34;am&#34;,
        &#34;is&#34;,
        &#34;are&#34;,
        &#34;was&#34;,
        &#34;were&#34;,
        &#34;be&#34;,
        &#34;been&#34;,
        &#34;being&#34;,
        &#34;have&#34;,
        &#34;has&#34;,
        &#34;had&#34;,
        &#34;having&#34;,
        &#34;do&#34;,
        &#34;does&#34;,
        &#34;did&#34;,
        &#34;doing&#34;,
        &#34;a&#34;,
        &#34;an&#34;,
        &#34;the&#34;,
        &#34;and&#34;,
        &#34;but&#34;,
        &#34;if&#34;,
        &#34;or&#34;,
        &#34;because&#34;,
        &#34;as&#34;,
        &#34;until&#34;,
        &#34;while&#34;,
        &#34;of&#34;,
        &#34;at&#34;,
        &#34;by&#34;,
        &#34;for&#34;,
        &#34;with&#34;,
        &#34;about&#34;,
        &#34;against&#34;,
        &#34;between&#34;,
        &#34;into&#34;,
        &#34;through&#34;,
        &#34;during&#34;,
        &#34;before&#34;,
        &#34;after&#34;,
        &#34;above&#34;,
        &#34;below&#34;,
        &#34;to&#34;,
        &#34;from&#34;,
        &#34;up&#34;,
        &#34;down&#34;,
        &#34;in&#34;,
        &#34;out&#34;,
        &#34;on&#34;,
        &#34;off&#34;,
        &#34;over&#34;,
        &#34;under&#34;,
        &#34;again&#34;,
        &#34;further&#34;,
        &#34;then&#34;,
        &#34;once&#34;,
        &#34;here&#34;,
        &#34;there&#34;,
        &#34;when&#34;,
        &#34;where&#34;,
        &#34;why&#34;,
        &#34;how&#34;,
        &#34;all&#34;,
        &#34;any&#34;,
        &#34;both&#34;,
        &#34;each&#34;,
        &#34;few&#34;,
        &#34;more&#34;,
        &#34;most&#34;,
        &#34;other&#34;,
        &#34;some&#34;,
        &#34;such&#34;,
        &#34;no&#34;,
        &#34;nor&#34;,
        &#34;not&#34;,
        &#34;only&#34;,
        &#34;own&#34;,
        &#34;same&#34;,
        &#34;so&#34;,
        &#34;than&#34;,
        &#34;too&#34;,
        &#34;very&#34;,
        &#34;s&#34;,
        &#34;t&#34;,
        &#34;can&#34;,
        &#34;will&#34;,
        &#34;just&#34;,
        &#34;don&#34;,
        &#34;don&#39;t&#34;,
        &#34;should&#34;,
        &#34;should&#39;ve&#34;,
        &#34;now&#34;,
        &#34;d&#34;,
        &#34;ll&#34;,
        &#34;m&#34;,
        &#34;o&#34;,
        &#34;re&#34;,
        &#34;ve&#34;,
        &#34;y&#34;,
        &#34;ain&#34;,
        &#34;aren&#34;,
        &#34;aren&#39;t&#34;,
        &#34;couldn&#34;,
        &#34;couldn&#39;t&#34;,
        &#34;didn&#34;,
        &#34;didn&#39;t&#34;,
        &#34;doesn&#34;,
        &#34;doesn&#39;t&#34;,
        &#34;hadn&#34;,
        &#34;hadn&#39;t&#34;,
        &#34;hasn&#34;,
        &#34;hasn&#39;t&#34;,
        &#34;haven&#34;,
        &#34;haven&#39;t&#34;,
        &#34;isn&#34;,
        &#34;isn&#39;t&#34;,
        &#34;ma&#34;,
        &#34;mightn&#34;,
        &#34;mightn&#39;t&#34;,
        &#34;mustn&#34;,
        &#34;mustn&#39;t&#34;,
        &#34;needn&#34;,
        &#34;needn&#39;t&#34;,
        &#34;shan&#34;,
        &#34;shan&#39;t&#34;,
        &#34;shouldn&#34;,
        &#34;shouldn&#39;t&#34;,
        &#34;wasn&#34;,
        &#34;wasn&#39;t&#34;,
        &#34;weren&#34;,
        &#34;weren&#39;t&#34;,
        &#34;won&#34;,
        &#34;won&#39;t&#34;,
        &#34;wouldn&#34;,
        &#34;wouldn&#39;t&#34;,
    ] + [&#34;,&#34;, &#34;$&#34;]

    # Use a masking technique to remove stopwords from tokens
    mask = list(map(lambda word: word not in stopwords, tokens))

    token_indices_no_stopwords = list(filter(lambda i: mask[i], range(len(tokens))))
    tokens_no_stopwords = [tokens[i] for i in token_indices_no_stopwords]

    # return tokens without stopwords and corresponding indices
    return tokens_no_stopwords, token_indices_no_stopwords


def search(
    query: str, max_num_results: int, session=scoped_session(sessionmaker(bind=engine))
) -&gt; list:
    &#34;&#34;&#34;
    Returns the top `max_num_results` relevant results based on the query given
    &#34;&#34;&#34;

    # preprocess the query
    query, _ = preprocess_text(query)

    # find the total number of documents, which will be used when computing the
    # inverse document frequency
    num_docs = int(list(session.execute(&#34;SELECT COUNT(*) from problem&#34;))[0][0])

    # initialize scores, the list of relevance scores based on which the query
    # results are displayed
    scores = defaultdict(float)

    # initialize query_tf, the list that maintains the term_frequency of the _query document_
    query_tf = defaultdict(int)

    # update query_tf based on term occurence
    for term in query:
        query_tf[term] += 1

    for term in query:
        # Find the row in the term dictionary corresponding to the current term
        term_dictionary_query = session.query(TermDictionary).filter_by(term=term).all()
        if len(term_dictionary_query) == 0:
            # term not present in term dictionary
            continue
        # grab the term attributes
        term_id = term_dictionary_query[0].term_id
        inverted_index_query = (
            session.query(InvertedIndex).filter_by(term_id=term_id).all()
        )
        term_document_frequency = inverted_index_query[0].document_frequency
        term_tf_idf = query_tf[term] * np.log(num_docs / term_document_frequency)
        term_postings_list = eval(
            session.query(InvertedIndex)
            .filter_by(term_id=term_id)
            .all()[0]
            .posting_list
        )

        # for each problem the term is present in, update the scores based on its frequency
        # in the document and the inverse document frequency
        for problem_id in term_postings_list:
            term_problem_tf = term_postings_list[problem_id]
            scores[problem_id] += term_problem_tf * term_tf_idf

    # normalize the scores by dividing each element by the corresponding document length
    for problem_id in scores:
        problem_length = (
            session.query(Problem)
            .filter_by(problem_id=problem_id)
            .all()[0]
            .problem_length
        )
        scores[problem_id] = scores[problem_id] / problem_length

    # sort the scores
    scores = sorted(scores, key=scores.get, reverse=True)

    # return the top scores
    return scores[: min(num_docs, max_num_results)]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="decipher.framework.search_utils.preprocess_text"><code class="name flex">
<span>def <span class="ident">preprocess_text</span></span>(<span>content: str) -> (<class 'list'>, <class 'list'>)</span>
</code></dt>
<dd>
<div class="desc"><p>Preprocesses the text.
Performs stemming and tokenization.
Returns tokens without stopwords and their corresponding indices.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/chiefsan/deCiPher/blob/c4ddeace7d1cdb139754478aa0e9415b06e073b4/decipher/framework/search_utils.py#L20-L225" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def preprocess_text(content: str) -&gt; (list, list):
    &#34;&#34;&#34;
    Preprocesses the text.
    Performs stemming and tokenization. 
    Returns tokens without stopwords and their corresponding indices.
    &#34;&#34;&#34;

    # convert the entire content to lowercase
    content = content.lower()

    # tokenize using the function nltk.word_tokenize
    tokens = nltk.word_tokenize(content)

    # define the stopwords (not downloading it like normal using nltk.download
    # because there was an issue during heroku deployment (resolved now).
    # &#39;$&#39; and &#39;,&#39; are stopwords specifically in this domain (observed when viewing the database)
    stopwords = [
        &#34;i&#34;,
        &#34;me&#34;,
        &#34;my&#34;,
        &#34;myself&#34;,
        &#34;we&#34;,
        &#34;our&#34;,
        &#34;ours&#34;,
        &#34;ourselves&#34;,
        &#34;you&#34;,
        &#34;you&#39;re&#34;,
        &#34;you&#39;ve&#34;,
        &#34;you&#39;ll&#34;,
        &#34;you&#39;d&#34;,
        &#34;your&#34;,
        &#34;yours&#34;,
        &#34;yourself&#34;,
        &#34;yourselves&#34;,
        &#34;he&#34;,
        &#34;him&#34;,
        &#34;his&#34;,
        &#34;himself&#34;,
        &#34;she&#34;,
        &#34;she&#39;s&#34;,
        &#34;her&#34;,
        &#34;hers&#34;,
        &#34;herself&#34;,
        &#34;it&#34;,
        &#34;it&#39;s&#34;,
        &#34;its&#34;,
        &#34;itself&#34;,
        &#34;they&#34;,
        &#34;them&#34;,
        &#34;their&#34;,
        &#34;theirs&#34;,
        &#34;themselves&#34;,
        &#34;what&#34;,
        &#34;which&#34;,
        &#34;who&#34;,
        &#34;whom&#34;,
        &#34;this&#34;,
        &#34;that&#34;,
        &#34;that&#39;ll&#34;,
        &#34;these&#34;,
        &#34;those&#34;,
        &#34;am&#34;,
        &#34;is&#34;,
        &#34;are&#34;,
        &#34;was&#34;,
        &#34;were&#34;,
        &#34;be&#34;,
        &#34;been&#34;,
        &#34;being&#34;,
        &#34;have&#34;,
        &#34;has&#34;,
        &#34;had&#34;,
        &#34;having&#34;,
        &#34;do&#34;,
        &#34;does&#34;,
        &#34;did&#34;,
        &#34;doing&#34;,
        &#34;a&#34;,
        &#34;an&#34;,
        &#34;the&#34;,
        &#34;and&#34;,
        &#34;but&#34;,
        &#34;if&#34;,
        &#34;or&#34;,
        &#34;because&#34;,
        &#34;as&#34;,
        &#34;until&#34;,
        &#34;while&#34;,
        &#34;of&#34;,
        &#34;at&#34;,
        &#34;by&#34;,
        &#34;for&#34;,
        &#34;with&#34;,
        &#34;about&#34;,
        &#34;against&#34;,
        &#34;between&#34;,
        &#34;into&#34;,
        &#34;through&#34;,
        &#34;during&#34;,
        &#34;before&#34;,
        &#34;after&#34;,
        &#34;above&#34;,
        &#34;below&#34;,
        &#34;to&#34;,
        &#34;from&#34;,
        &#34;up&#34;,
        &#34;down&#34;,
        &#34;in&#34;,
        &#34;out&#34;,
        &#34;on&#34;,
        &#34;off&#34;,
        &#34;over&#34;,
        &#34;under&#34;,
        &#34;again&#34;,
        &#34;further&#34;,
        &#34;then&#34;,
        &#34;once&#34;,
        &#34;here&#34;,
        &#34;there&#34;,
        &#34;when&#34;,
        &#34;where&#34;,
        &#34;why&#34;,
        &#34;how&#34;,
        &#34;all&#34;,
        &#34;any&#34;,
        &#34;both&#34;,
        &#34;each&#34;,
        &#34;few&#34;,
        &#34;more&#34;,
        &#34;most&#34;,
        &#34;other&#34;,
        &#34;some&#34;,
        &#34;such&#34;,
        &#34;no&#34;,
        &#34;nor&#34;,
        &#34;not&#34;,
        &#34;only&#34;,
        &#34;own&#34;,
        &#34;same&#34;,
        &#34;so&#34;,
        &#34;than&#34;,
        &#34;too&#34;,
        &#34;very&#34;,
        &#34;s&#34;,
        &#34;t&#34;,
        &#34;can&#34;,
        &#34;will&#34;,
        &#34;just&#34;,
        &#34;don&#34;,
        &#34;don&#39;t&#34;,
        &#34;should&#34;,
        &#34;should&#39;ve&#34;,
        &#34;now&#34;,
        &#34;d&#34;,
        &#34;ll&#34;,
        &#34;m&#34;,
        &#34;o&#34;,
        &#34;re&#34;,
        &#34;ve&#34;,
        &#34;y&#34;,
        &#34;ain&#34;,
        &#34;aren&#34;,
        &#34;aren&#39;t&#34;,
        &#34;couldn&#34;,
        &#34;couldn&#39;t&#34;,
        &#34;didn&#34;,
        &#34;didn&#39;t&#34;,
        &#34;doesn&#34;,
        &#34;doesn&#39;t&#34;,
        &#34;hadn&#34;,
        &#34;hadn&#39;t&#34;,
        &#34;hasn&#34;,
        &#34;hasn&#39;t&#34;,
        &#34;haven&#34;,
        &#34;haven&#39;t&#34;,
        &#34;isn&#34;,
        &#34;isn&#39;t&#34;,
        &#34;ma&#34;,
        &#34;mightn&#34;,
        &#34;mightn&#39;t&#34;,
        &#34;mustn&#34;,
        &#34;mustn&#39;t&#34;,
        &#34;needn&#34;,
        &#34;needn&#39;t&#34;,
        &#34;shan&#34;,
        &#34;shan&#39;t&#34;,
        &#34;shouldn&#34;,
        &#34;shouldn&#39;t&#34;,
        &#34;wasn&#34;,
        &#34;wasn&#39;t&#34;,
        &#34;weren&#34;,
        &#34;weren&#39;t&#34;,
        &#34;won&#34;,
        &#34;won&#39;t&#34;,
        &#34;wouldn&#34;,
        &#34;wouldn&#39;t&#34;,
    ] + [&#34;,&#34;, &#34;$&#34;]

    # Use a masking technique to remove stopwords from tokens
    mask = list(map(lambda word: word not in stopwords, tokens))

    token_indices_no_stopwords = list(filter(lambda i: mask[i], range(len(tokens))))
    tokens_no_stopwords = [tokens[i] for i in token_indices_no_stopwords]

    # return tokens without stopwords and corresponding indices
    return tokens_no_stopwords, token_indices_no_stopwords</code></pre>
</details>
</dd>
<dt id="decipher.framework.search_utils.search"><code class="name flex">
<span>def <span class="ident">search</span></span>(<span>query: str, max_num_results: int, session=&lt;sqlalchemy.orm.scoping.scoped_session object&gt;) -> list</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the top <code>max_num_results</code> relevant results based on the query given</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/chiefsan/deCiPher/blob/c4ddeace7d1cdb139754478aa0e9415b06e073b4/decipher/framework/search_utils.py#L228-L293" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def search(
    query: str, max_num_results: int, session=scoped_session(sessionmaker(bind=engine))
) -&gt; list:
    &#34;&#34;&#34;
    Returns the top `max_num_results` relevant results based on the query given
    &#34;&#34;&#34;

    # preprocess the query
    query, _ = preprocess_text(query)

    # find the total number of documents, which will be used when computing the
    # inverse document frequency
    num_docs = int(list(session.execute(&#34;SELECT COUNT(*) from problem&#34;))[0][0])

    # initialize scores, the list of relevance scores based on which the query
    # results are displayed
    scores = defaultdict(float)

    # initialize query_tf, the list that maintains the term_frequency of the _query document_
    query_tf = defaultdict(int)

    # update query_tf based on term occurence
    for term in query:
        query_tf[term] += 1

    for term in query:
        # Find the row in the term dictionary corresponding to the current term
        term_dictionary_query = session.query(TermDictionary).filter_by(term=term).all()
        if len(term_dictionary_query) == 0:
            # term not present in term dictionary
            continue
        # grab the term attributes
        term_id = term_dictionary_query[0].term_id
        inverted_index_query = (
            session.query(InvertedIndex).filter_by(term_id=term_id).all()
        )
        term_document_frequency = inverted_index_query[0].document_frequency
        term_tf_idf = query_tf[term] * np.log(num_docs / term_document_frequency)
        term_postings_list = eval(
            session.query(InvertedIndex)
            .filter_by(term_id=term_id)
            .all()[0]
            .posting_list
        )

        # for each problem the term is present in, update the scores based on its frequency
        # in the document and the inverse document frequency
        for problem_id in term_postings_list:
            term_problem_tf = term_postings_list[problem_id]
            scores[problem_id] += term_problem_tf * term_tf_idf

    # normalize the scores by dividing each element by the corresponding document length
    for problem_id in scores:
        problem_length = (
            session.query(Problem)
            .filter_by(problem_id=problem_id)
            .all()[0]
            .problem_length
        )
        scores[problem_id] = scores[problem_id] / problem_length

    # sort the scores
    scores = sorted(scores, key=scores.get, reverse=True)

    # return the top scores
    return scores[: min(num_docs, max_num_results)]</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="pdoc Home" href="https://chiefsan.github.io/deCiPher/">
<img src="https://upload.wikimedia.org/wikipedia/commons/6/61/Searchtool.svg" alt=""> deCiPher
</a>
</header>
<div class="gcse-search" style="height: 70px"
data-as_oq="site:chiefsan.github.io inurl:github.com/chiefsan/deCiPher"
data-gaCategoryParameter="decipher.framework.search_utils">
</div>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="decipher.framework" href="index.html">decipher.framework</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="decipher.framework.search_utils.preprocess_text" href="#decipher.framework.search_utils.preprocess_text">preprocess_text</a></code></li>
<li><code><a title="decipher.framework.search_utils.search" href="#decipher.framework.search_utils.search">search</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>